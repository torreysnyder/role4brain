"""
Complete Example: Training Role Learner on TPDN L2R Embeddings with Evaluation (Gumbel-Softmax)

Pipeline:
1) Load L2R TPDN embeddings (generated by tpdn_test4role.py)
2) Train a ROLE-style tensor-product encoder to reproduce those embeddings
3) Train a Transformer decoder on the *true* TPDN embeddings
4) Evaluate "substitution accuracy" by feeding ROLE's approximation to the decoder
   -> Here we discretize role assignment via Gumbel-Softmax (soft or straight-through hard)

This follows the ROLE evaluation idea of passing approximations through the decoder (substitution accuracy),
but uses Gumbel-Softmax rather than snapping or discrete two-stage training. See ROLE paper §5.1.  # (paper)
"""

import sys
import json
import random
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch import optim
import matplotlib.pyplot as plt
from tqdm import tqdm
import torch.nn.functional as F
import csv

# Import the simplified role learning encoder
from role_learning_tensor_product_encoder import RoleLearningTensorProductEncoder  # uses RoleAssignment LSTM etc.  # (local)

# --------------------
# Utilities
# --------------------

def set_seed(seed):
    use_cuda = torch.cuda.is_available()
    print(f"CUDA available: {use_cuda}")
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if use_cuda:
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
    device = torch.device("cuda" if use_cuda else "cpu")
    return device


class RoleLearningInstance(object):
    """Instance for role learning that includes target representations"""

    def __init__(self, filler, target_rep, target_output=None, target_roles=None):
        self.filler = filler  # Input sequence (LongTensor) shape [1, seq_len]
        self.target_rep = target_rep  # Target representation from TPDN (FloatTensor) shape [1, rep_dim]
        self.target_output = target_output  # Target output sequence for decoder training (LongTensor) [1, seq_len]
        self.target_roles = target_roles  # Optional: known roles for analysis (LongTensor) [1, seq_len]


# --------------------
# Decoder
# --------------------

class PositionalEncoding(nn.Module):
    """Standard sinusoidal positional encoding for transformer"""
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class SimpleTransformerDecoder(nn.Module):
    """
    Transformer decoder that takes an *encoding vector* (the TPDN/ROLE encoding) and autoregressively produces an output sequence.
    """
    def __init__(self, encoding_dim, output_vocab_size, max_seq_len=20,
                 d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1):
        super().__init__()
        self.encoding_dim = encoding_dim
        self.output_vocab_size = output_vocab_size
        self.max_seq_len = max_seq_len
        self.d_model = d_model

        self.encoding_proj = nn.Linear(encoding_dim, d_model)
        self.output_embedding = nn.Embedding(output_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,
            dropout=dropout, batch_first=True
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.output_proj = nn.Linear(d_model, output_vocab_size)

    def forward(self, encoding, target_seq=None, teacher_forcing_ratio=1.0):
        """
        encoding: (batch, encoding_dim)
        target_seq: (batch, seq_len) of token IDs. If None -> autoregressive generation
        """
        batch_size = encoding.size(0)
        memory = self.encoding_proj(encoding).unsqueeze(1)  # (batch, 1, d_model)

        if target_seq is not None and random.random() < teacher_forcing_ratio:
            seq_len = target_seq.size(1)
            tgt_embedded = self.output_embedding(target_seq)  # (batch, seq_len, d_model)
            tgt_embedded = self.pos_encoder(tgt_embedded)
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(encoding.device)
            decoder_output = self.transformer_decoder(tgt_embedded, memory, tgt_mask=tgt_mask)
            return self.output_proj(decoder_output)  # (batch, seq_len, vocab)
        else:
            # autoregressive
            SOS, EOS = 1, 2  # convention used elsewhere in the script
            cur = torch.full((batch_size, 1), SOS, dtype=torch.long, device=encoding.device)
            outs = []
            for _ in range(self.max_seq_len):
                tgt_emb = self.output_embedding(cur)
                tgt_emb = self.pos_encoder(tgt_emb)
                tgt_mask = nn.Transformer.generate_square_subsequent_mask(cur.size(1)).to(encoding.device)
                dec_out = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)
                next_logits = self.output_proj(dec_out[:, -1:, :])  # (batch, 1, vocab)
                outs.append(next_logits)
                next_tok = next_logits.argmax(dim=-1)
                cur = torch.cat([cur, next_tok], dim=1)
                if (next_tok == EOS).all():
                    break
            return torch.cat(outs, dim=1) if outs else torch.zeros(batch_size, 0, self.output_vocab_size, device=encoding.device)


# --------------------
# Data loading (TPDN outputs to supervise ROLE and train decoder)
# --------------------

def load_tpdn_outputs(filename, device, load_targets=True):
    """
    JSON structure expected per item:
      {
        "filler_ids": [...],              # discrete filler tokens (0..100)
        "model_output": [...],            # target encoding vector produced by TPDN (float list)
        "target_output": [...],           # optional: output token seq for decoder training
        "role_ids": [...]                 # optional: reference role IDs for analysis
      }
    """
    print(f"Loading TPDN outputs from {filename}...")
    try:
        with open(filename, 'r') as f:
            outputs = json.load(f)
    except FileNotFoundError:
        print(f"ERROR: File {filename} not found. Generate it with tpdn_test4role.py first.")
        sys.exit(1)
    if not outputs:
        print("ERROR: No data in file.")
        sys.exit(1)

    print(f"Loaded {len(outputs)} instances")
    data = []
    for entry in outputs:
        filler_ids = entry['filler_ids']
        target_output_embedding = entry['model_output']
        target_output_seq = entry.get('target_output', filler_ids) if load_targets else None
        role_ids = entry.get('role_ids', None)

        filler_t = Variable(torch.LongTensor(filler_ids), requires_grad=False).unsqueeze(0)
        target_emb_t = Variable(torch.FloatTensor(target_output_embedding), requires_grad=False).unsqueeze(0)
        target_seq_t = Variable(torch.LongTensor(target_output_seq), requires_grad=False).unsqueeze(0) if target_output_seq else None
        role_t = Variable(torch.LongTensor(role_ids), requires_grad=False).unsqueeze(0) if role_ids is not None else None

        filler_t = filler_t.to(device)
        target_emb_t = target_emb_t.to(device)
        if target_seq_t is not None: target_seq_t = target_seq_t.to(device)
        if role_t is not None: role_t = role_t.to(device)

        data.append(RoleLearningInstance(filler_t, target_emb_t, target_seq_t, role_t))

    print(f"Created {len(data)} role learning instances")
    if data:
        print(f"  Sequence length: {len(outputs[0]['filler_ids'])}")
        print(f"  Target representation size: {len(outputs[0]['model_output'])}")
    return data


# --------------------
# Gumbel-Softmax
# --------------------

def gumbel_softmax_sample(logits, temperature=1.0, hard=False):
    """
    logits: (*, num_classes)
    returns: same shape, probability simplex (or straight-through one-hot if hard=True)
    """
    eps = 1e-20
    gumbel = -torch.log(-torch.log(torch.rand_like(logits) + eps) + eps)
    y = (logits + gumbel) / max(temperature, 1e-8)
    y_soft = F.softmax(y, dim=-1)
    if not hard:
        return y_soft
    y_hard = torch.zeros_like(y_soft)
    y_hard.scatter_(-1, y_soft.argmax(dim=-1, keepdim=True), 1.0)
    # straight-through
    return y_hard - y_soft.detach() + y_soft


def _find_role_embedding_weight(role_model):
    """
    Best-effort to retrieve the role-embedding matrix (num_roles x role_dim)
    from the role assigner. We try a few common attribute names.
    """
    ra = role_model.role_assigner
    # common names: role_embedding, role_embed, R
    for name in ["role_embedding", "role_embed", "R", "roles", "embedding"]:
        if hasattr(ra, name):
            obj = getattr(ra, name)
            # If it's an Embedding module
            if isinstance(obj, nn.Embedding):
                return obj.weight
            # If it's a tensor that already holds weights
            if torch.is_tensor(obj) and obj.dim() == 2:
                return obj
    # as a last resort, scan attributes for an Embedding
    for k, v in ra.__dict__.items():
        if isinstance(v, nn.Embedding):
            return v.weight
    raise AttributeError("Could not locate role-embedding weights inside role_assigner.")


@torch.no_grad()
def encode_with_gumbel(role_model, filler, temperature=1.0, hard=False):
    """
    Produce ROLE encoding with Gumbel-Softmax-discretized role assignments.

    Steps:
      1) Get filler embeddings (and optional squeeze)
      2) Get role predictions (prob or logits) from role assigner
      3) Convert to logits if needed; apply Gumbel-Softmax
      4) Recompute role vectors via a @ R_weight
      5) Bind fillers x roles via the model's binding layer; apply final linear if present
    """
    device = filler.device
    # 1) filler embeddings
    fillers_emb = role_model.filler_embedding(filler)  # (batch, seq, filler_dim)
    if getattr(role_model, "embed_squeeze", False):
        fillers_emb = role_model.embedding_squeeze_layer(fillers_emb)

    # 2) role predictions & role-embedding matrix
    #    role_assigner returns (roles_embedded, role_predictions)
    #    role_predictions typically has shape (seq_len, batch, num_roles)
    _, role_preds = role_model.role_assigner(filler)
    # bring to (batch, seq, roles)
    if role_preds.dim() == 3:
        role_preds_bsr = role_preds.transpose(0, 1)  # (batch, seq, roles)
    else:
        raise RuntimeError("Unexpected role_predictions shape: {}".format(role_preds.shape))

    # 3) logits -> Gumbel-Softmax
    # If predictions look like probs, turn into logits
    # Heuristic: if all entries are in [0,1] and rows sum ~1, treat as probs.
    rp = role_preds_bsr
    rp_sum = rp.sum(dim=-1, keepdim=True).clamp_min(1e-8)
    looks_like_probs = bool(torch.all((rp >= -1e-6) & (rp <= 1 + 1e-6))) and torch.allclose(rp_sum, torch.ones_like(rp_sum), atol=1e-3, rtol=1e-3)
    logits = torch.log(rp.clamp(min=1e-8)) if looks_like_probs else rp  # if already logits, use as-is

    role_weights = gumbel_softmax_sample(logits, temperature=temperature, hard=hard)  # (batch, seq, roles)

    # 4) role vectors via embedding matrix
    R_w = _find_role_embedding_weight(role_model)  # (num_roles, role_dim)
    # roles_embedded: (batch, seq, role_dim) = (batch, seq, roles) @ (roles, dim)
    roles_emb = torch.einsum("bsr,rd->bsd", role_weights, R_w)

    # 5) bind + optional final linear
    bound = role_model.sum_layer(fillers_emb, roles_emb)  # expected to return (batch, *, binder_dim)
    if getattr(role_model, "has_last", 0) == 1:
        bound = role_model.last_layer(bound)
    return bound  # (batch, encoding_dim)


# --------------------
# Training helpers
# --------------------

def batchify(data, batch_size):
    """Group data into mini-batches (list of tuples)."""
    batches = []
    for bi in range(0, len(data), batch_size):
        batch_data = data[bi:min(bi + batch_size, len(data))]
        first = batch_data[0]
        cur_filler = first.filler
        cur_target = first.target_rep
        cur_output = first.target_output if first.target_output is not None else None
        for inst in batch_data[1:]:
            cur_filler = torch.cat((cur_filler, inst.filler), 0)
            cur_target = torch.cat((cur_target, inst.target_rep), 0)
            if cur_output is not None and inst.target_output is not None:
                cur_output = torch.cat((cur_output, inst.target_output), 0)
        batches.append((cur_filler, cur_target, cur_output) if cur_output is not None else (cur_filler, cur_target))
    return batches


def train_epoch(model, train_batches, optimizer, criterion, batch_size, use_regularization=False):
    """One epoch of ROLE training against MSE to target TPDN encodings."""
    model.train()
    random.shuffle(train_batches)
    epoch_losses, epoch_reg_losses = [], []

    for bi in tqdm(range(0, len(train_batches), batch_size), desc="Training"):
        batch = train_batches[bi:min(bi + batch_size, len(train_batches))]
        if not batch: continue
        optimizer.zero_grad()
        total_loss, total_reg = 0, 0

        for batch_item in batch:
            filler_in = batch_item[0]
            target_out = batch_item[1]

            enc_out, role_predictions = model(filler_in, None)
            recon_loss = criterion(enc_out, target_out)
            total_loss += recon_loss

            if use_regularization:
                one_hot_loss, l2_loss, unique_loss = model.get_regularization_loss(role_predictions)
                reg_loss = one_hot_loss + l2_loss + unique_loss
                total_reg += reg_loss
                total_loss += reg_loss

        total_loss.backward()
        optimizer.step()
        epoch_losses.append(recon_loss.item())
        if use_regularization:
            epoch_reg_losses.append(total_reg.item() if total_reg != 0 else 0)

    avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else float('inf')
    avg_reg = sum(epoch_reg_losses) / len(epoch_reg_losses) if epoch_reg_losses else 0
    return avg_loss, avg_reg


def train_decoder(decoder, data, optimizer, criterion, batch_size, n_epochs=50, patience=10):
    """
    Train transformer decoder on *true* TPDN embeddings to reconstruct the output sequences.
    mirrors the ROLE paper “substitution accuracy” setup where the decoder is trained on the target encodings,
    and later tested by feeding ROLE approximations.  # (paper)
    """
    print("\n" + "=" * 80)
    print("TRAINING TRANSFORMER DECODER")
    print("=" * 80)

    n_total = len(data)
    n_train = int(0.85 * n_total)
    train_data = data[:n_train]
    valid_data = data[n_train:]

    print(f"Training decoder on {len(train_data)} sequences")
    print(f"Validation: {len(valid_data)} sequences")

    best_loss = float('inf')
    patience_counter = 0
    train_losses, valid_losses = [], []

    for epoch in range(n_epochs):
        decoder.train()
        epoch_loss, n_batches = 0, 0
        indices = torch.randperm(len(train_data))

        for i in tqdm(range(0, len(train_data), batch_size), desc=f"Epoch {epoch + 1}"):
            batch_indices = indices[i:min(i + batch_size, len(train_data))]
            batch_enc = torch.stack([train_data[idx].target_rep.squeeze(0) for idx in batch_indices])
            batch_tgt = torch.stack([train_data[idx].target_output.squeeze(0) for idx in batch_indices])

            optimizer.zero_grad()
            logits = decoder(batch_enc, batch_tgt[:, :-1], teacher_forcing_ratio=1.0)
            loss = criterion(logits.reshape(-1, logits.size(-1)), batch_tgt[:, 1:].reshape(-1))
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item(); n_batches += 1

        avg_train = epoch_loss / max(1, n_batches)
        train_losses.append(avg_train)

        # validation
        decoder.eval()
        v_loss, v_batches = 0, 0
        with torch.no_grad():
            for j in range(0, len(valid_data), batch_size):
                chunk = valid_data[j:min(j + batch_size, len(valid_data))]
                v_enc = torch.stack([inst.target_rep.squeeze(0) for inst in chunk])
                v_tgt = torch.stack([inst.target_output.squeeze(0) for inst in chunk])
                v_logits = decoder(v_enc, v_tgt[:, :-1], teacher_forcing_ratio=1.0)
                loss = criterion(v_logits.reshape(-1, v_logits.size(-1)), v_tgt[:, 1:].reshape(-1))
                v_loss += loss.item(); v_batches += 1
        avg_valid = v_loss / max(1, v_batches)
        valid_losses.append(avg_valid)

        print(f"Epoch {epoch + 1}/{n_epochs}: Train Loss={avg_train:.4f}, Valid Loss={avg_valid:.4f}")
        if avg_valid < best_loss:
            best_loss = avg_valid; patience_counter = 0
            torch.save(decoder.state_dict(), "best_decoder_model.pt")
            print("  → New best decoder saved!")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("\nEarly stopping")
                break

    decoder.load_state_dict(torch.load("best_decoder_model.pt"))
    return decoder, train_losses, valid_losses


# --------------------
# Evaluation with Gumbel-Softmax
# --------------------
@torch.no_grad()
def evaluate(role_model, data):
    role_model.eval()
    mse = 0.0
    for inst in data:
        pred, _ = role_model(inst.filler, None)
        mse += torch.mean((pred - inst.target_rep) ** 2).item()
    return mse / max(1, len(data))

@torch.no_grad()
def analyze_role_assignments(model, data, num_samples=10):
    model.eval()
    print("\n" + "=" * 80)
    print("ROLE ASSIGNMENT ANALYSIS")
    print("=" * 80)
    for i in range(min(num_samples, len(data))):
        instance = data[i]
        _, role_predictions = model(instance.filler, None)
        role_pred_np = role_predictions.squeeze(1).cpu().numpy()  # (seq, roles)
        assigned_roles = np.argmax(role_pred_np, axis=1)
        max_probs = np.max(role_pred_np, axis=1)
        fillers = instance.filler.squeeze(0).cpu().numpy()
        if instance.target_roles is not None:
            l2r_roles = instance.target_roles.squeeze(0).cpu().numpy()
            match = np.array_equal(l2r_roles, assigned_roles)
            print(f"\nInstance {i}:")
            print(f"  Fillers:       {fillers}")
            print(f"  L2R roles:     {l2r_roles}")
            print(f"  Learned roles: {assigned_roles}")
            print(f"  Confidence:    {['%.3f' % p for p in max_probs]}")
            print(f"  Match L2R:     {match}")
        else:
            print(f"\nInstance {i}:")
            print(f"  Fillers:       {fillers}")
            print(f"  Learned roles: {assigned_roles}")
            print(f"  Confidence:    {['%.3f' % p for p in max_probs]}")


@torch.no_grad()
def evaluate_substitution_accuracy(role_model, decoder, data, gumbel_temp=1.0, use_hard_gumbel=False):
    """
    Substitution accuracy: feed ROLE’s (gumbelized) approximation into the trained decoder
    and check if the produced sequence matches the target (prefix exact match).
    """
    role_model.eval()
    decoder.eval()

    correct, total = 0, 0
    for inst in tqdm(data, desc=f"Eval (T={gumbel_temp:.3f}, hard={use_hard_gumbel})"):
        role_enc = encode_with_gumbel(
            role_model, inst.filler, temperature=gumbel_temp, hard=use_hard_gumbel
        )  # (1, encoding_dim)

        output_logits = decoder(role_enc, target_seq=None, teacher_forcing_ratio=0.0)
        pred_seq = output_logits.argmax(dim=-1).squeeze(0)  # (T_gen,)
        target_seq = inst.target_output.squeeze(0)

        L = min(pred_seq.size(0), target_seq.size(0))
        if torch.equal(pred_seq[:L], target_seq[:L]):
            correct += 1
        total += 1

    return (correct / total) if total > 0 else 0.0


# >>> NEW: annealed evaluation sweep
@torch.no_grad()
def evaluate_across_temps(
    role_model,
    decoder,
    data,
    T_init=1.0,
    T_min=0.1,
    decay=0.95,
    use_hard_gumbel=False,
    csv_path="sub_acc_vs_T.csv",
    png_path="sub_acc_vs_T.png",
):
    """
    Run substitution accuracy across a temperature schedule:
      T_{k+1} = max(T_min, T_k * decay)

    Saves a CSV and a PNG of accuracy vs. T.
    """
    assert 0 < decay <= 1.0, "decay must be in (0,1]"
    assert T_init > 0 and T_min > 0, "temperatures must be > 0"

    temps, accs = [], []
    T = float(T_init)
    step = 0

    # Guard against decay==1 infinite loop
    def _next_temp(cur):
        nxt = cur * decay
        if decay == 1.0:
            return T_min  # one more step to T_min, then exit
        return max(T_min, nxt)

    while True:
        acc = evaluate_substitution_accuracy(
            role_model, decoder, data, gumbel_temp=T, use_hard_gumbel=use_hard_gumbel
        )
        temps.append(T)
        accs.append(acc)
        print(f"  -> T={T:.4f} | hard={use_hard_gumbel} | substitution_acc={acc:.4f}")

        if T <= T_min + 1e-12:
            break
        T_next = _next_temp(T)
        if abs(T_next - T) < 1e-12:  # safety
            break
        T = T_next
        step += 1
        if step > 10_000:  # extreme safety
            break

    # Write CSV
    with open(csv_path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["temperature", "substitution_accuracy", "hard"])
        for t, a in zip(temps, accs):
            w.writerow([t, a, int(use_hard_gumbel)])
    print(f"Saved CSV: {csv_path}")

    # Plot
    plt.figure()
    plt.plot(temps, accs, marker="o")
    plt.gca().set_xscale("log")
    plt.gca().invert_xaxis()  # lower T on the right
    plt.xlabel("Gumbel temperature T (log scale)")
    plt.ylabel("Substitution accuracy")
    title_mode = "HARD (ST)" if use_hard_gumbel else "SOFT"
    plt.title(f"Decoder substitution accuracy vs. temperature — {title_mode}")
    plt.grid(True, linestyle="--", alpha=0.4)
    plt.tight_layout()
    plt.savefig(png_path, dpi=150)
    print(f"Saved plot: {png_path}")

    return list(zip(temps, accs))


# --------------------
# (Example) main orchestration
# --------------------

def main():
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--tpdn_json", type=str, required=True, help="JSON with TPDN outputs for ROLE/decoder")
    ap.add_argument("--epochs_role", type=int, default=50)
    ap.add_argument("--epochs_dec", type=int, default=50)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--gumbel_temp", type=float, default=0.5, help="One-shot eval temperature (non-annealed)")
    ap.add_argument("--hard_gumbel", action="store_true", help="Also run one-shot hard (ST) Gumbel at --gumbel_temp")

    # NEW: eval-only annealing controls
    ap.add_argument("--temp_init", type=float, default=1.0, help="Initial temperature for annealed eval sweep")
    ap.add_argument("--temp_min", type=float, default=0.1, help="Minimum temperature for annealed eval sweep")
    ap.add_argument("--temp_decay", type=float, default=0.95, help="Exponential decay factor per eval step")
    ap.add_argument("--no_soft_curve", action="store_true", help="Skip soft (non-ST) annealed curve")
    ap.add_argument("--no_hard_curve", action="store_true", help="Skip hard (ST) annealed curve")

    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    device = set_seed(args.seed)

    # 1) Load data
    data = load_tpdn_outputs(args.tpdn_json, device, load_targets=True)
    n_total = len(data)
    n_train = int(0.8 * n_total)
    train_set = data[:n_train]
    test_set = data[n_train:]

    # 2) Build ROLE model (sizes should match your TPDN output dims / vocab)
    num_fillers = 101  # tokens 0..100 (per generator)
    n_roles = 32
    filler_dim = 32
    role_dim = 16
    target_dim = data[0].target_rep.size(1)

    role_model = RoleLearningTensorProductEncoder(
        n_roles=n_roles,
        n_fillers=num_fillers,
        filler_dim=filler_dim,
        role_dim=role_dim,
        final_layer_width=target_dim,
        binder="tpr",
        role_learner_hidden_dim=64,
        bidirectional=False,
        num_layers=1,
        softmax_roles=True,
        one_hot_regularization_weight=1.0,
        l2_norm_regularization_weight=1.0,
        unique_role_regularization_weight=1.0,
    ).to(device)

    # 3) Train ROLE to match TPDN encodings
    optimizer = optim.Adam(role_model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()
    train_batches = batchify(train_set, args.batch_size)

    print("\nTraining ROLE to match TPDN encodings...")
    best = float('inf')
    for ep in range(args.epochs_role):
        avg_loss, _ = train_epoch(role_model, train_batches, optimizer, criterion, batch_size=1, use_regularization=True)
        val_mse = evaluate(role_model, test_set)
        print(f"ROLE Epoch {ep+1}/{args.epochs_role}: Train MSE={avg_loss:.6f} | Val MSE={val_mse:.6f}")
        if val_mse < best:
            best = val_mse
            torch.save(role_model.state_dict(), "best_role_model.pt")
            print("  → Saved best ROLE model")
    role_model.load_state_dict(torch.load("best_role_model.pt"))

    # 4) Train Transformer decoder on *true* TPDN encodings
    out_vocab = 103  # adjust if needed (includes PAD/SOS/EOS)
    decoder = SimpleTransformerDecoder(
        encoding_dim=target_dim, output_vocab_size=out_vocab, max_seq_len=data[0].target_output.size(1)
    ).to(device)

    dec_opt = optim.Adam(decoder.parameters(), lr=1e-3)
    dec_crit = nn.CrossEntropyLoss(ignore_index=0)
    decoder, _, _ = train_decoder(decoder, data, dec_opt, dec_crit, batch_size=args.batch_size,
                                  n_epochs=args.epochs_dec, patience=10)

    # 5a) One-shot substitution accuracy at a fixed T (soft + optional hard)
    acc_soft = evaluate_substitution_accuracy(role_model, decoder, test_set,
                                              gumbel_temp=args.gumbel_temp, use_hard_gumbel=False)
    print(f"\n[One-shot] Substitution accuracy (SOFT, T={args.gumbel_temp}): {acc_soft:.4f}")

    if args.hard_gumbel:
        acc_hard = evaluate_substitution_accuracy(role_model, decoder, test_set,
                                                  gumbel_temp=args.gumbel_temp, use_hard_gumbel=True)
        print(f"[One-shot] Substitution accuracy (HARD-ST, T={args.gumbel_temp}): {acc_hard:.4f}")

    # 5b) Annealed sweeps (eval-only)
    if not args.no_soft_curve:
        print("\n=== Annealed sweep (SOFT) ===")
        evaluate_across_temps(
            role_model, decoder, test_set,
            T_init=args.temp_init, T_min=args.temp_min, decay=args.temp_decay,
            use_hard_gumbel=False,
            csv_path="sub_acc_vs_T_soft.csv",
            png_path="sub_acc_vs_T_soft.png",
        )

    if not args.no_hard_curve:
        print("\n=== Annealed sweep (HARD-ST) ===")
        evaluate_across_temps(
            role_model, decoder, test_set,
            T_init=args.temp_init, T_min=args.temp_min, decay=args.temp_decay,
            use_hard_gumbel=True,
            csv_path="sub_acc_vs_T_hard.csv",
            png_path="sub_acc_vs_T_hard.png",
        )


if __name__ == "__main__":
    main()
